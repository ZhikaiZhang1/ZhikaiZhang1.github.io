<!DOCTYPE HTML>
<script>
  function showMore() {

    var listData = Array.prototype.slice.call(document.querySelectorAll('#dataList li:not(.shown)')).slice(0, 9);

    for (var i = 0; i < listData.length; i++) {
      listData[i].className = 'shown';
      listData[i].style.display = 'list-item';
    }
    switchButtons();
  }


  function switchButtons() {
    var hiddenElements = Array.prototype.slice.call(document.querySelectorAll('#dataList li:not(.shown)'));
    if (hiddenElements.length == 0) {
      document.getElementById('moreButton').style.display = 'none';
    }
    else {
      document.getElementById('moreButton').style.display = 'block';
    }

    var shownElements = Array.prototype.slice.call(document.querySelectorAll('#dataList li:not(.hidden)'));
    if (shownElements.length == 0) {
      document.getElementById('lessButton').style.display = 'none';
    }
    else {
      document.getElementById('lessButton').style.display = 'block';
    }
  }

  onload = function () {
    showMore();
  }
</script>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhikai (Logan) Zhang</title>

  <meta name="author" content="Zhikai (Logan) Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Table of Contents</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          display: flex;
          flex-direction: row;
      }
      #toc {
          width: 200px;
          padding: 20px;
          background-color: #f5f5f5;
          position: fixed;
          top: 0;
          left: 0;
          height: 100%;
          overflow-y: auto;
      }
      #content {
          margin-left: 240px;
          padding: 20px;
      }
      h2 {
          margin-top: 40px;
      }
  </style>
</head>
<body>
  <div id="toc">
      <h2>Table of Contents</h2>
      <ul>
        <li><a href="#top_toc">Top</a></li>
          <li><a href="#Research_Interests">Research Interests</a></li>
          <li><a href="#Robot_Platforms">Robot Platforms</a></li>
          <li><a href="#Research_Projects">Research/Projects</a></li>
          <li><a href="#Professional_Experiences">Experiences</a></li>
      </ul>
  </div>
<body>

<body>
  <table
    style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">

                    <name id="top_toc">Zhikai (Logan) Zhang</name>
                  </p>
                  <p>I recently graduated from my MSME-R program in the <a href="https://www.meche.engineering.cmu.edu/">Department of Mechanical
                     Engineering</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>
                    I did my research at the <a href="https://biorobotics.ri.cmu.edu/index.php">Biorobotics Lab</a> and advised by
                    Professor <a href="https://www.ri.cmu.edu/ri-faculty/howie-choset/">Howie Choset</a>. </p>

                    <p>Currently I am working in Biorobotics Lab as a Research Engineer, working on Pipe inspection robot control/prototyping and modular robot learning</p>
                    <p>I was previously an embedded software intern at <a href="https://www.garmin.com/en-US/company/">Garmin Ltd.</a></p>

                  <p> I received my BASc. degree from <a href="https://ece.ubc.ca/">Department of Eletrical and Computer
                    Engineering</a>, <a href"https://www.ubc.ca/">University of British Columbia</a> in 2022. </p>
                  <p style="text-align:center">
                    <a href="zhikaiz@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                    <!-- <a href="https://scholar.google.com/citations?user=BNnGdCAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp -->
                    <a href="https://github.com/ZhikaiZhang1">Github</a> &nbsp/&nbsp
                    <a href="images/profile/Resume.pdf">Resume</a>     
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/profile/Logan_robot.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/profile/Logan_robot.jpg" class="hoverZoomLink">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
             </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <h2 id="Research_Interests">Research/Career Interests</h2>
          <!-- <heading>Research Interests</heading> -->
          <p>
            I am deeply passionate about the field of robotics, with a particular focus on modular robots, robot learning, control and planning.
            My current research is centered around the Modular robot distributed control, reinforcement Learning and planning over control.
            I hope to explore more on control and planning in robotics, as well as the novel and relatively less explored field of design automation. 
          </p> 
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <td style="padding: 10px; width: 100%; text-align: left;">
        <h2 id="Robot_Platforms">Robot Platforms</h2>

        <!-- <heading>Robot Platforms</heading> -->
      <style>
          /* Style for the container div */
          .image-container {
              display: flex; /* Use flexbox to arrange items */
              flex-wrap: wrap; /* Wrap items to the next line if needed */
              justify-content: center; /* Center align items horizontally */
              gap: 10px; /* Gap between images */
          }
  
          /* Style for each image div */
          .image-item {
              flex: 0 0 calc(33.33% - 20px); /* Each image takes up 33.33% of the container width minus some spacing */
              max-width: calc(33.33% - 20px); /* Maximum width for each image */
              height: 200px; /* Fixed height for each image container */
              position: relative; /* Enable relative positioning for inner image */
              overflow: hidden; /* Hide overflowing image */
          }
  
          /* Responsive style for smaller screens */
          @media (max-width: 768px) {
              .image-item {
                  flex: 0 0 calc(50% - 20px); /* Each image takes up 50% of the container width minus some spacing on smaller screens */
                  max-width: calc(50% - 20px); /* Maximum width for each image on smaller screens */
              }
          }
  
          /* Style for images inside each image div */
          .image-item img {
              width: 100%; /* Make images fill the entire width of their container */
              height: 100%; /* Make images fill the entire height of their container */
              object-fit: contain; /* Scale the image while preserving aspect ratio, fitting it within the container */
              position: absolute; /* Position the image absolutely within its container */
              top: 0;
              left: 0;
          }
        /* Style for caption */
        .caption {
            position: absolute;
            bottom: 0;
            left: 0;
            width: 100%;
            background-color: rgba(0, 0, 0, 0.7); /* Semi-transparent black background for caption */
            color: white; /* Text color for caption */
            padding: 8px;
            box-sizing: border-box;
            text-align: center;
            font-size: 14px;
        }
      </style>
  </head>
  <body>
  
  <div class="image-container">
      <!-- First row of images -->
      <div class="image-item">
          <img src="images/Eigenbot/eigenbot_shot.png" alt="Image 1">
          <div class="caption">Modular Robot - Eigenbot</div>
      </div>
      <div class="image-item">
          <img src="images/LongJump/go1.webp" alt="Image 2">
          <div class="caption">Unitree Go1</div>
      </div>
      <div class="image-item">
          <img src="images/ballbot/ballbot_arm_no_light.png" alt="Image 3">
          <div class="caption">CMU ballbot</div>
      </div>
  
      <!-- Second row of images -->
      <div class="image-item">
          <img src="images/iiwa/iiwa_300x300.avif" alt="Image 4">
          <div class="caption">KUKA IIWA</div>
      </div>
      <div class="image-item">
          <img src="images/Thunderbots/thunderbots_shot.jpg" alt="Image 5">
          <div class="caption">Robocup Robots - Thunderbots</div>
      </div>
      <div class="image-item">
          <img src="images/robot_arm/Robot_v2.png" alt="Image 6">
          <div class="caption">self made 6DoF robot arm</div>
      </div>
  </div>
  
  </body>
</table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <h2 id="Research_Projects">Research/Projects</h2>
          <!-- <heading>Research/Projects</heading> -->
        </td>
      </tr>
    </tbody>
  </table>
  
  <!-- <hr style="width:50%;vertical-align:middle"> -->
    <!-- Eigenbot RL -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        
        <td style="padding:5px;width:30%;vertical-align:middle;text-align: center;">
          <video width="600" muted autoplay loop>
            <source src="Videos/eigenbot/Eigenbot RL.mp4" type="video/mp4">
          </video>  
          <p style="text-align: center;">Eigenbot walking and running on Terrain in Simulation</p>
        </td>

        <td style="padding:20px;width:80%;vertical-align:middle">
          <!-- <a href="https://zhl355.github.io/ERL_EAST/"> -->
            <papertitle>Modular RL on Modular Robot
            </papertitle>
          </a>
          <br>
          <strong>Zhikai (Logan) Zhang</strong>, 
          <a href="https://www.linkedin.com/in/shankruth-balasubramaniyam-160b83239/">shankruth balasubramaniyam</a>,
          <a href="https://www.ri.cmu.edu/ri-people/lu-li/">Lu Li</a>,
          <a href="https://www.ri.cmu.edu/ri-faculty/howie-choset/">Howie Choset</a>
          <br>
          <!-- <a href="https://zhl355.github.io/ERL_EAST/">website</a> /
          <a href="https://arxiv.org/pdf/2310.01363.pdf">arXiv</a> -->
          
          <p>Current RL training pipeline gives a centralised end to end policy, which requires heavy tuning and is subjected to unstable performances and local minimum. Such policy needs to be re-trained for different 
            types of robots and is hard to collect enough data, which is lacking in the robotics field. In order to move closer to a robot foundation model, we propose to modularize different 
            components of the robot, eg. legs, vision, arms etc. to be pre-trained on variety of tasks. Such modular networks can thus be combined into different configurations and only needs 
            simpler fine tuning. We hope to simplify the RL design cycle and data collection pipeline with this appraoch.
            We use reinforcement learning (PPO) to train a walking/running policy for a modular legged robot. Currently we have done the following in simulation<br>
            1. successfully trained <br> 
            2. Used <a href="https://manipulation-locomotion.github.io/">Regularized Online Adaptation (ROA)</a> for Sim2Real transfer of priviliged states<br> 
            3. distillation of a depth encoder-based policy to use depth image in place of priviliged sandots<br> 

            Next steps: <br>
            1. Testing our policy on Hadware that has action delays and less agile actuators<br>
            2. Modularize the policy to be leg/joint specific for locomotion and manipulation<br>
            3. Testing a modularized policy ith different configurations (eg. different number of legs, loco-manipulation configurations) <br>
          </p>   

        </td>
      </tbody>
    </table>
    <hr style="width:50%;vertical-align:middle">

    <!-- Eigenbot -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        
        <td style="padding:5px;width:30%;vertical-align:middle;text-align: center;">
          <video width="400" muted autoplay loop>
            <source src="images/Eigenbot/4-legs-hardware.mp4" type="video/mp4">
          </video>  
          <p style="text-align: center;">4 leg walking configuration</p>

             
          <!-- <source src="Videos/eigenbot/flat_good.mp4" type="video/mp4">         -->
          <img src='images/Eigenbot/flat_good_1_5_speed.gif' width="400">
          <p style="text-align: center;">Hardware walking on flat terrain</p>    
          
          <img src='images/Eigenbot/unstable_block_sim.gif' width="200">
          <p style="text-align: center;">Simulation walking on unstable block</p> 
               
          <!-- <img src='images/Eigenbot/maze_gvn.png' width="300">  
          <p style="text-align: center;">robot navigate adaptively through maze</p>                    -->

        </td>
  
        <td style="padding:20px;width:80%;vertical-align:middle">
          <!-- <a href="https://zhl355.github.io/ERL_EAST/"> -->
            <papertitle>Distributed Neural Control on Modular Robot (ICRA 2025 Acceptance)
            </papertitle>
          </a>
          <br>
          <strong>Zhikai (Logan) Zhang</strong>, 
          <a href="https://www.ri.cmu.edu/ri-people/henry-kou/">Henry Kou</a>,
          <a href="https://www.linkedin.com/in/siqiguo047/">Siqi Guo</a>, 
          <a href="https://www.ri.cmu.edu/ri-people/lu-li/">Lu Li</a>,
          <a href="https://www.ri.cmu.edu/ri-faculty/howie-choset/">Howie Choset</a>
          <br>
          <!-- <a href="https://zhl355.github.io/ERL_EAST/">website</a> /
          <a href="https://arxiv.org/pdf/2310.01363.pdf">arXiv</a> -->
          
          <p>Conventional robotic control methods analyze the entire robot's system to consider every degree of freedom at the same time and are often computationally expensive. However, animals do not think how to walk with their brain, rather, computational units used for control are distributed into the neuron pools in their body parts. In a distributed implementation, we can reduce the computational load and create adaptive behaviors with only local sensory feedback at a Low computational complexity. This is especially relevant for modular robots where each module consists of low power MCUs that cannot compute complex control algorithms online. Our contribution in this study are three folds:<br> 
            1. A biologically inspired distributed control framework independent of central module that can adapt to reasonable gaits terrain<br> 
            2. A low computational cost hardware implementation on a truly distributed platform through messaging passing and local proprioceptive feedback<br> 
            3. Our results confirms the following hypothesis for modular robotics locomotion - local proprioceptive feedback is enough for achieving stable locomotion and a adaptive behavior doesn't require complex models such as optimal control and reinforcement learning. <br> 
            Through simulation as well as hardware experiments, we are able to showcase the prowess of distributed hardware implementation as well as neural controller's capability of generating legged locomotion gaits adaptive to various terrains, modulated only by local proprioception. We hope to extend this controller to generalize modular robot control. 
            
          </p>

          <div style="text-align: center;">
            <!-- <video width="600" muted autoplay loop>
              <source src="images/Eigenbot/terrain_good_gif.gif" type="video/mp4">
            </video> -->
            <img src='images/Eigenbot/terrain_good_gif.gif' width="600">
            <p style="text-align: center;">distributed controller with 6 legs on hardware with hill terrain</p>
          </div>      

        </td>
      </tbody>
    </table>
    <hr style="width:50%;vertical-align:middle">
    <!-- planning over control -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <td style="padding:20px;width:30%;vertical-align:middle;text-align: center;">
          <img src='images/LongJump/Cost_predictor_energy.gif' width=200">
          <p style="text-align: center;">Energy Cost Prediction when approaching an obstacle</p>            
          <img src='images/LongJump/Sim2Real.gif' width="200">
          <p style="text-align: center;">Sim2Real Work</p>   
        </td>
  
        <td style="padding:20px;width:80%;vertical-align:middle">
          <a href="images/LongJump/16_782_Longjump_Project_Report.pdf">
            <papertitle>Path Planning for a Quadruped in Arbitrary 3-Dimensional Environments
            </papertitle>
          </a>
          <br>
          <strong>Zhikai (Logan) Zhang</strong>, <a href="https://trunc8.github.io/">Siddarth Saha</a>,
          <a href="https://www.ri.cmu.edu/ri-people/sayan-mondal/">Sayan Mondal</a>,
          <a href="https://www.linkedin.com/in/grsu/"> Gregory Su</a>
          <br>
          
          <p>Real time events are performed with instinct gained from repeated practice, while long term planning optimization requires thinking and understanding of the task at hand.
            This is analogous to fast and agile movements vs optimal path planning in robotics.
            Navigating challenging terrains demands a nuanced understanding of the robot's versatile capabilities. Highly agile mobile 
            robots endowed with complex locomotion skills, including climbing, jumping, and walking can often better handle such
            environments than traditional wheels, but planning for such systems can often be much more difficult. This paper introduces a
            comprehensive framework for real-time autonomous navigation of such motions by determining path costs using a neural network-
            based cost predictor by leveraging elevation maps and A* path planning. We demonstrate in simulation that this system can plan
            paths for multiple different cost functions, such as minimum energy or minimum time, depending on the specified criteria.
          </p>

          <div style="text-align: center;">
            <img src='images/LongJump/planned_path.png' width=300">
            <p style="text-align: center;">Planned Path based on Costr predidtor</p>
            <img src='images/LongJump/sim_execution.gif' width=300">
            <p style="text-align: center;">Planned Path Execution in Simulation</p>
          </div>
        </td>
      </tbody>
    </table>
    <hr style="width:50%;vertical-align:middle">

    
  <!-- Ballbot -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <td style="padding:20px;width:30%;vertical-align:middle;text-align: center;">
        <img src='images/ballbot/navigation_no_arms_8_shape.gif' width=300">
        <p style="text-align: center;">Ballbot avoid obstacle real time</p>            
        <img src='images/ballbot/pushing_constrained.gif' width="220">
        <p style="text-align: center;">Ballbot pushing on wall</p>   
      </td>

      <td style="padding:20px;width:80%;vertical-align:middle">
        <a href="images/ballbot/16745_ballbot_report.pdf">
          <papertitle>Whole-body Trajectory Optimization and Tracking for Agile Maneuvers for a Single-Spherical-Wheeled Balancing Mobile Manipulator
          </papertitle>
        </a>
        <br>
        <strong>Zhikai (Logan) Zhang</strong>,
        Juan Alvarez-Padilla, <a href="https://www.ri.cmu.edu/ri-people/haoru-xue/">Haoru Xue</a>,
        <a href="https://www.ri.cmu.edu/ri-people/christian-berger/">Christian Berger</a>,
        <a href="https://www.ri.cmu.edu/ri-people/sayan-mondal/">Sayan Mondal</a>,
        <a href="https://www.ri.cmu.edu/ri-people/haoru-xue/">Haoru Xue</a>
        <br>
        
        <!-- <br>
        <a href="https://thaipduong.github.io/refgovham/">website</a> /
        <!-- <a href="https://thaipduong.github.io/refgovham/">video</a> / -->
        <!-- <a href="https://arxiv.org/abs/2207.10840">arXiv</a> --> --> <a href="https://docs.google.com/presentation/d/15cSchb-V0RFb_xRScefn_o5W5IGBx3DGX8WWrD1mBvQ/edit?usp=sharing"> slides </a>
        
        <p>Several ballbots have been developed, yet only a handful have been equipped with arms to enhance their maneuverability and manipulability. 
          The incorporation of 7- DOF arms to the CMU ballbot has presented challenges in balancing and navigation due to the constantly changing center of mass. 
          This project aims to propose a control strategy that incorporates the arms dynamics. Our approach is to use a simplified whole-body dynamics model, with only the shoulder and elbow joints moving for each arm. 
          This reduces the number of states and accelerates convergence. We focused on two specific tasks: navigation (straight and curved paths) and pushing against a wall. 
          Trajectories were generated using direct collocation for the navigation task and hybrid contact trajectory optimization for pushing the wall. 
          A time-variant linear quadratic regulator (TVLQR) was designed to track the trajectories. The resulting trajectories were tracked with a mean-average error of less than 4 cm, 
          even for the more complex path. These experiments represent an initial step towards unlocking the full potential of ballbots in dynamic and interactive environments. 
          <br>Supplementary information, including code and animations, can be found at https://github.com/jrapudg/ocrl ballbot navigation project.
        </p>
      </td>
      
    </tbody>
  </table>

  <hr style="width:50%;vertical-align:middle">

    <!-- LfD -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <td style="padding:5px;width:30%;vertical-align:middle;text-align: center;">
          <img src='images/iiwa/iiwa_draw.gif' width="200">
          <p style="text-align: center;">KUKA IIWA arm motion planned with RRT-connect</p>  
            
          <img src='images/iiwa/Hololens_interface.png' width="350">
          <p style="text-align: center;">Hololens AR interface developed</p>     
        </td>
  
        <td style="padding:20px;width:80%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2310.13083.pdf">
            <papertitle>How Can Everyday Users Efficiently Teach Robots by Demonstrations? 
            </papertitle>
          </a>
          <br>
          <a href="https://scholar.google.ca/citations?user=GUOlf-IAAAAJ&hl=en">Maram Sakr </a>, 
          <strong>Zhikai (Logan) Zhang</strong>, 
          <a href="https://www.linkedin.com/in/benji-li/?originalSubdomain=ca">Benjamin Li</a>, 
          <a href="https://dfp.ubc.ca/people/trainees/haomiao-zhang">Haomiao Zhang</a>, 
          <a href="https://scholar.google.ca/citations?user=ze-QdW0AAAAJ&hl=en">H.F. Machiel Van der Loos</a>, 
          <a href="https://scholar.google.com/citations?user=sL0KJlQAAAAJ&hl=en">Dana Kulic</a>, 
          <a href="https://scholar.google.com/citations?user=s7yPf64AAAAJ&hl=en">Elizabeth Croft</a>
        <br>
          <p>Learning from Demonstration (LfD) is a framework that allows lay users to easily program robots. However, the efficiency of robot learning and the robot's ability 
            to generalize to task variations hinges upon the quality and quantity of the provided demonstrations. 
            Our objective is to guide human teachers to furnish more effective demonstrations, thus facilitating efficient robot learning. 
            To achieve this, we propose to use a measure of uncertainty, namely task-related information entropy, as a criterion for suggesting informative demonstration 
            examples to human teachers to improve their teaching skills. In a conducted experiment (N=24), an augmented reality (AR)-based guidance system was employed to 
            train novice users to produce additional demonstrations from areas with the highest entropy within the workspace. These novice users were trained for a few trials to 
            teach the robot a generalizable task using a limited number of demonstrations. Subsequently, the users' performance after training was assessed first on the same task 
            (retention) and then on a novel task (transfer) without guidance. The results indicated a substantial improvement in robot learning efficiency from the teacher's 
            demonstrations, with an improvement of up to 198% observed on the novel task. Furthermore, the proposed approach was compared to a state-of-the-art heuristic rule and 
            found to improve robot learning efficiency by 210% compared to the heuristic rule.</p>
  
        

          <div style="text-align: center;">
            <img src='images/iiwa/TPGMM_unsupervised_learning.png' width="350">
            <p style="text-align: center;">TPGMM online unsupervised Learning</p>
          </div>

        </td>
        </tr>
      </tbody>
    </table>

    
    <hr style="width:50%;vertical-align:middle">
    <!-- UBC Thunderbots -->
  <!-- OJCSYS 2022 Robust Safe Flight SE(3) -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <td style="padding:20px;width:30%;vertical-align:middle;text-align: center;">
        <img src='images/Thunderbots/planning_diagram.png' width="300">
        <p style="text-align: center;">Path Planning</p>
        <img src='images/Thunderbots/First_Place.jpg' width="300">
        <p style="text-align: center;">Winning First Place in Robotcup 2019</p>
        <img src='images/Thunderbots/firmware_struct.png' width="300">
        <p style="text-align: center;">Control Archetecture Overview</p>
        
      </td>

      <td style="padding:20px;width:80%;vertical-align:middle">
        <a href="https://ssl.robocup.org/wp-content/uploads/2020/03/2020_TDP_UBC_Thunderbots.pdf">
          <papertitle>UBCThundrbots Student design Team Eletrical Co-Lead For Robocup Competition
          </papertitle>
        </a>
        <br>
        
        <p>As the Eletrical Sub-team Co-Lead, I spearheaded multiple projects, including Eletrical system redesign, Wifii system integration and FPGA Updates. 
          Technical Work during this Time includes Wifi firmware integration, Robot firmware development, part of Robot Main board design, FPGA intrgration with Wifi system on SPI protocol, Planning Software as well as solenoid curve fitting for better chipping and kicking performance.
          <br>
          Through a Collective Team Effort, we were able to achieve first place in Robocup 2019.  
        </p>
      

      <div style="text-align: center;">
        <img src='images/Thunderbots/2019_robocup_goal.gif' width="300">
        <p style="text-align: center;">Goal by chipping in Robocup 2019</p>
        <img src='images/Thunderbots/dribble_steal_shoot.gif' width="300">
        <p style="text-align: center;">Dribbler ball steal and shoot for goal with motion Primitive</p>
      </div>
    </td>
    </tbody>
  </table>


  <hr style="width:50%;vertical-align:middle">

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <td style="padding:20px;width:30%;vertical-align:middle;text-align: center;">
        <img src='images/robot_arm/Robot_v2.png' width="100">
        <p style="text-align: center;">Robot Arm Version 2</p>
        <img src='images/robot_arm/ROS_Integration.gif' width="200">
        <p style="text-align: center;">ROS Integration with Robot Arm Version 2</p>
        
      </td>

      <td style="padding:20px;width:80%;vertical-align:middle">
        <!-- <a href="https://ssl.robocup.org/wp-content/uploads/2020/03/2020_TDP_UBC_Thunderbots.pdf"> -->
          <papertitle>6DoF Robot arm build/Control
          </papertitle>
        </a>
        <br>
        
        <p>In this personal Project, I have worked with the following:
            <br>
            1. Mechanical, electrical design and Built a robot arm with 3D printed parts & off the shelf motors
            <br>
            2. 6DOF robot dynamics analysis and PD + gravity control, implementation on the physical robot
            <br>
            3. Trjectory generation with 6Dof Arm and control firmware on STM32 platform
        </p>
      

      <div style="text-align: center;">
        <img src='images/robot_arm/Robot_arm_with_trajectory.gif' width="300">
        <p style="text-align: center;">Robot arm version 1 with trajectory control, smoother than set point control</p>
        <img src='images/robot_arm/Roobot_set_point_control.gif' width="300">
        <p style="text-align: center;">Robot arm version 1 with set point control</p>
      </div>
    </td>
    </tbody>
  </table>
    

    
  <hr style="width:50%;vertical-align:middle">
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <h2 id="Professional_Experiences">Experiences</h2>

          <!-- <heading>Professional Experiences</heading> -->
        </td>
      </tr>
    </tbody>
  </table>
    
  <!-- ARPA-E control -->
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      
      <td style="padding:5px;width:30%;vertical-align:middle;text-align: center;">
        <video width="600" muted autoplay loop>
          <source src="Videos/ARPA-E/kalma filter_reflective_pipe.mp4" type="video/mp4">
        </video>  
        <p style="text-align: center;">CV based Yaw estimation in noisy reflective pipe</p>
      <!-- </td>
      <td style="padding:5px;width:30%;vertical-align:middle;text-align: center;"> -->
        <video width="600" muted autoplay loop>
          <source src="Videos/ARPA-E/Deployment_data_kalman_filter.mp4" type="video/mp4">
        </video>  
        <p style="text-align: center;">CV based Yaw estimation in field deployment pipe (less noisy)</p>
      </td>

      <td style="padding:20px;width:80%;vertical-align:middle">
        <!-- <a href="https://zhl355.github.io/ERL_EAST/"> -->
          <papertitle>Research Engineer at CMU <a href="https://biorobotics.ri.cmu.edu/index.php">Biorobotics Lab</a> and <a href="https://www.jprobotics.ai/ ">Pipe Force</a>
          </papertitle>
        </a>
        <br>
        
        <p>Pipe Force specializes in intelligent in-pipe surveying and digitizing pipe assets using robotics and AI, providing actionable data that helps infrastructure owners minimize 
          maintenance costs due to pipe failure. This startup stemmed from the Biorobotics Lab<br>
          I use Computer Vision + IMU sensor fusion for localisation of the robot's odometery in the pipe environment using Kalman filter. Two pipe environments are tested using this 
          method, as shown in the video.<br>
          The estimated states are used in a controller for adjusting the robot's orientation, tested in simulation and harware.
        </p>   
        <div style="text-align: center;">
          <video width="600" muted autoplay loop>
            <source src="Videos/ARPA-E/ARPA-E_control_Sim_Hardware.mp4" type="video/mp4">
          </video>
          <p style="text-align: center;">Orienatation Control Simulation vs Hardware</p>
        </div> 

      </td>
    </tbody>
  </table>
  <hr style="width:50%;vertical-align:middle">

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <td style="padding:20px;width:30%;vertical-align:middle;text-align: center;">
        <img src='images/Garmin/ANT_chip_modules.jpg' width="220">
        <p style="text-align: center;">ANT protocl chip module</p>
        <img src='images/Garmin/LLV4.jpg' width="220">
        <p style="text-align: center;">Lidar-lite V4 wireless sensor</p>
        
      </td>

      <td style="padding:20px;width:80%;vertical-align:middle">
        <a href="https://www.garmin.com/en-US/">
          <papertitle>Garmin Embedded Software Internship
          </papertitle>
        </a>
        <br>
        
        <p>As an Embedded Software Intern, I worked on vrious projects in Garmin, including <a href="https://developer.garmin.com/ant-program/overview/">ANT network</a> (Low energy Wireless network used internally in Garmin), 
          large scale <a href="https://forums.garmin.com/developer/connect-iq/b/news-announcements/posts/bluetooth-mesh-networking-with-connect-iq">BLE mesh</a> testing and firmware debugging/testing, <a href="https://www.garmin.com/en-US/p/610275">Lidar-lite V4 sensor</a>,
          and other frimware testing scripts such as connection tests and battery data analysis. 
        </p>
      

      <div style="text-align: center;">
        <img src='images/Garmin/ANT_logo.png' width="300">
        <p style="text-align: center;">ANT protocol</p>
        <img src='images/Garmin/BLE_mesh.png' width="300">
        <p style="text-align: center;">BLE Mesh Network</p>
      </div>
    </td>
    </tbody>
  </table>


  <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <p style="text-align:right;font-size:small;">Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
        </p>
    </tbody>
  </table> -->
</body>

</html>